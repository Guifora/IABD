{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CkMrsp85SQ4"
      },
      "source": [
        "# EJERCICIOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "LuecDqKh5SRT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit, current_date, year, monotonically_increasing_id,  avg, min, coalesce\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"pyspark_rdd\").getOrCreate()\n",
        "#spark = SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxPbn7ct5SRW"
      },
      "source": [
        "## EJERCICIO 0\n",
        "En un documento word haz una lista de las diferentes operaciones con una breve descripción de lo que hace y un ejemplo de como se utiliza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2ywlD0T5SRX"
      },
      "source": [
        "## EJERCICIO 1\n",
        "Realiza las siguientes operaciones:\n",
        "* Importa el csv de \"data/WorldCupPlayers.csv\" (que deduzca el esquema)\n",
        "* Visualiza los datos\n",
        "* ¿Que tipo de datos contiene cada variable?\n",
        "* ¿Cuantos registros hay?\n",
        "* Obtén los principales estadísticos de Position\n",
        "* Selecciona y muestra los \"Team initials\" diferentes que hay ¿Cuántos hay?\n",
        "* ¿Cuantos partidos con el ID de 1096 ha habido?\n",
        "* Muestra los datos donde la posicion haya sido C y el evento sea G40\n",
        "* Utiliza Spark SQL para mostras los registros donde el MatchID sea mayor o igual a 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QnmFQq55SRX",
        "outputId": "cbfb95e7-dd96-40bf-c9fe-80a07a6bad62"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv(\"recursos/WorldCupPlayers.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------+-------------+-------------------+-------+------------+-----------------+--------+---------+\n",
            "|RoundID|MatchID|Team Initials|         Coach Name|Line-up|Shirt Number|      Player Name|Position|    Event|\n",
            "+-------+-------+-------------+-------------------+-------+------------+-----------------+--------+---------+\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|      Alex THEPOT|      GK|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|  Oscar BONFIGLIO|      GK|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0| Marcel LANGILLER|    NULL|     G40'|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|     Juan CARRENO|    NULL|     G70'|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|  Ernest LIBERATI|    NULL|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|     Rafael GARZA|       C|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|  Andre MASCHINOT|    NULL|G43' G87'|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|    Hilario LOPEZ|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|  Etienne MATTLER|    NULL|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|   Dionisio MEJIA|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|     Marcel PINEL|    NULL|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|     Felipe ROSAS|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|  Alex VILLAPLANE|       C|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|     Manuel ROSAS|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|   Lucien LAURENT|    NULL|     G19'|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|        Jose RUIZ|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|   Marcel CAPELLE|    NULL|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|  Alfredo SANCHEZ|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|Augustin CHANTREL|    NULL|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|   Efrain AMEZCUA|    NULL|     NULL|\n",
            "+-------+-------+-------------+-------------------+-------+------------+-----------------+--------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- RoundID: integer (nullable = true)\n",
            " |-- MatchID: integer (nullable = true)\n",
            " |-- Team Initials: string (nullable = true)\n",
            " |-- Coach Name: string (nullable = true)\n",
            " |-- Line-up: string (nullable = true)\n",
            " |-- Shirt Number: integer (nullable = true)\n",
            " |-- Player Name: string (nullable = true)\n",
            " |-- Position: string (nullable = true)\n",
            " |-- Event: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37784"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-----+\n",
            "|Position|count|\n",
            "+--------+-----+\n",
            "|    NULL|33641|\n",
            "|      GK| 2441|\n",
            "|       C| 1510|\n",
            "|     GKC|  192|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy(\"Position\").count().orderBy(\"count\", ascending=False).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-----+\n",
            "|Team Initials|count|\n",
            "+-------------+-----+\n",
            "|          BRA| 2403|\n",
            "|          ITA| 1843|\n",
            "|          ARG| 1807|\n",
            "|          ENG| 1378|\n",
            "|          FRG| 1364|\n",
            "|          FRA| 1344|\n",
            "|          ESP| 1317|\n",
            "|          NED| 1210|\n",
            "|          MEX| 1190|\n",
            "|          URU| 1159|\n",
            "|          GER| 1088|\n",
            "|          SWE|  992|\n",
            "|          BEL|  944|\n",
            "|          YUG|  799|\n",
            "|          SUI|  753|\n",
            "|          CHI|  748|\n",
            "|          USA|  729|\n",
            "|          HUN|  704|\n",
            "|          KOR|  695|\n",
            "|          POL|  688|\n",
            "+-------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy(\"Team Initials\").count().orderBy(\"count\", ascending=False).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.filter(df.MatchID == 1096).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------+-------------+--------------------+-------+------------+----------------+--------+-----+\n",
            "|RoundID|MatchID|Team Initials|          Coach Name|Line-up|Shirt Number|     Player Name|Position|Event|\n",
            "+-------+-------+-------------+--------------------+-------+------------+----------------+--------+-----+\n",
            "|    201|   1089|          PAR|DURAND LAGUNA Jos...|      S|           0|Luis VARGAS PENA|       C| G40'|\n",
            "|    429|   1175|          HUN|  DIETZ Karoly (HUN)|      S|           0|   Gyorgy SAROSI|       C| G40'|\n",
            "+-------+-------+-------------+--------------------+-------+------------+----------------+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter((df.Position == \"C\") & (df.Event == \"G40'\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------+-------------+-------------------+-------+------------+-----------------+--------+---------+\n",
            "|RoundID|MatchID|Team Initials|         Coach Name|Line-up|Shirt Number|      Player Name|Position|    Event|\n",
            "+-------+-------+-------------+-------------------+-------+------------+-----------------+--------+---------+\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|      Alex THEPOT|      GK|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|  Oscar BONFIGLIO|      GK|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0| Marcel LANGILLER|    NULL|     G40'|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|     Juan CARRENO|    NULL|     G70'|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|  Ernest LIBERATI|    NULL|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|     Rafael GARZA|       C|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|  Andre MASCHINOT|    NULL|G43' G87'|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|    Hilario LOPEZ|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|  Etienne MATTLER|    NULL|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|   Dionisio MEJIA|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|     Marcel PINEL|    NULL|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|     Felipe ROSAS|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|  Alex VILLAPLANE|       C|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|     Manuel ROSAS|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|   Lucien LAURENT|    NULL|     G19'|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|        Jose RUIZ|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|   Marcel CAPELLE|    NULL|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|  Alfredo SANCHEZ|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|Augustin CHANTREL|    NULL|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|   Efrain AMEZCUA|    NULL|     NULL|\n",
            "+-------+-------+-------------+-------------------+-------+------------+-----------------+--------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Registrar el DataFrame como una vista temporal\n",
        "df.createOrReplaceTempView(\"WorldCupPlayers\")\n",
        "\n",
        "# Ejecutar la consulta SQL\n",
        "result = spark.sql(\"SELECT * FROM WorldCupPlayers WHERE MatchID >= 20\")\n",
        "\n",
        "# Mostrar los resultados\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwlXPat85SRc"
      },
      "source": [
        "## EJERCICIO 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySES8yYI5SRc"
      },
      "source": [
        "A partir del archivo nombres.json, crea un DataFrame y realiza las siguientes operaciones:\n",
        "\n",
        "1. Crea una nueva columna (columna Mayor30) que indique si la persona es mayor de 30 años.\n",
        "2. Crea una nueva columna (columna FaltanJubilacion) que calcule cuantos años le faltan para jubilarse (supongamos que se jubila a los 67 años)\n",
        "3. Crea una nueva columna (columna Apellidos) que contenga XYZ (puedes utilizar la función lit)\n",
        "4. Elimina las columna Mayor30 y Apellidos.\n",
        "5. Crea una nueva columna (columna AnyoNac) con el año de nacimiento de cada persona (puedes utilizar la función current_date).\n",
        "6. Añade un id incremental para cada fila (campo Id) y haz que al hacer un show se vea en primer lugar (puedes utilizar la función monotonically_increasing_id) seguidos del Nombre, Edad, AnyoNac, FaltaJubilacion y Ciudad\n",
        "\n",
        "Al realizar los seis pasos, el resultado del DataFrame será similar a :\n",
        "``````\n",
        "+---+-------+----+-------+----------------+--------+\n",
        "| Id|Nombre |Edad|AnyoNac|FaltanJubilacion|  Ciudad|\n",
        "+---+-------+----+-------+----------------+--------+\n",
        "|  0|  Aitor|  45|   1977|              22|   Elche|\n",
        "|  1| Marina|  14|   2008|              53|Alicante|\n",
        "|  2|  Laura|  19|   2003|              48|   Elche|\n",
        "|  3|  Sonia|  45|   1977|              22|    Aspe|\n",
        "|  4|  Pedro|null|   null|            null|   Elche|\n",
        "+---+-------+----+-------+----------------+--------+\n",
        "``````"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpLo_k6g5SRc",
        "outputId": "4a0cebf1-589b-4053-ad62-db2740038016"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+----+------+\n",
            "|  Ciudad|Edad|Nombre|\n",
            "+--------+----+------+\n",
            "|   Elche|  45| Aitor|\n",
            "|Alicante|  14|Marina|\n",
            "|   Elche|  19| Laura|\n",
            "|    Aspe|  45| Sonia|\n",
            "|   Elche|NULL| Pedro|\n",
            "+--------+----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_nombres = spark.read.json(\"recursos/nombres.json\")\n",
        "df_nombres.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+----+------+-------+\n",
            "|  Ciudad|Edad|Nombre|Mayor30|\n",
            "+--------+----+------+-------+\n",
            "|   Elche|  45| Aitor|   true|\n",
            "|Alicante|  14|Marina|  false|\n",
            "|   Elche|  19| Laura|  false|\n",
            "|    Aspe|  45| Sonia|   true|\n",
            "|   Elche|NULL| Pedro|   NULL|\n",
            "+--------+----+------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "# Agregar la columna Mayor30\n",
        "df_nombres = df_nombres.withColumn(\"Mayor30\", expr(\"Edad > 30\"))\n",
        "df_nombres.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+----+------+-------+----------------+\n",
            "|  Ciudad|Edad|Nombre|Mayor30|FaltanJubilacion|\n",
            "+--------+----+------+-------+----------------+\n",
            "|   Elche|  45| Aitor|   true|              22|\n",
            "|Alicante|  14|Marina|  false|              53|\n",
            "|   Elche|  19| Laura|  false|              48|\n",
            "|    Aspe|  45| Sonia|   true|              22|\n",
            "|   Elche|NULL| Pedro|   NULL|            NULL|\n",
            "+--------+----+------+-------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Agregar la columna FaltanJubilacion\n",
        "df_nombres = df_nombres.withColumn(\"FaltanJubilacion\", expr(\"67 - Edad\"))\n",
        "\n",
        "# Mostrar los resultados\n",
        "df_nombres.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+----+------+-------+----------------+---------+\n",
            "|  Ciudad|Edad|Nombre|Mayor30|FaltanJubilacion|Apellidos|\n",
            "+--------+----+------+-------+----------------+---------+\n",
            "|   Elche|  45| Aitor|   true|              22|      XYZ|\n",
            "|Alicante|  14|Marina|  false|              53|      XYZ|\n",
            "|   Elche|  19| Laura|  false|              48|      XYZ|\n",
            "|    Aspe|  45| Sonia|   true|              22|      XYZ|\n",
            "|   Elche|NULL| Pedro|   NULL|            NULL|      XYZ|\n",
            "+--------+----+------+-------+----------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import lit\n",
        "\n",
        "df_nombres = df_nombres.withColumn(\"Apellidos\", lit(\"XYZ\"))\n",
        "df_nombres.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+----+------+----------------+\n",
            "|  Ciudad|Edad|Nombre|FaltanJubilacion|\n",
            "+--------+----+------+----------------+\n",
            "|   Elche|  45| Aitor|              22|\n",
            "|Alicante|  14|Marina|              53|\n",
            "|   Elche|  19| Laura|              48|\n",
            "|    Aspe|  45| Sonia|              22|\n",
            "|   Elche|NULL| Pedro|            NULL|\n",
            "+--------+----+------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_nombres = df_nombres.drop(\"Mayor30\", \"Apellidos\")\n",
        "df_nombres.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+----+------+----------------+-------+\n",
            "|  Ciudad|Edad|Nombre|FaltanJubilacion|AnyoNac|\n",
            "+--------+----+------+----------------+-------+\n",
            "|   Elche|  45| Aitor|              22|   1980|\n",
            "|Alicante|  14|Marina|              53|   2011|\n",
            "|   Elche|  19| Laura|              48|   2006|\n",
            "|    Aspe|  45| Sonia|              22|   1980|\n",
            "|   Elche|NULL| Pedro|            NULL|   NULL|\n",
            "+--------+----+------+----------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import current_date, year\n",
        "\n",
        "df_nombres = df_nombres.withColumn(\"AnyoNac\", year(current_date()) - df_nombres[\"Edad\"])\n",
        "df_nombres.show() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+----+-------+----------------+--------+\n",
            "| Id|Nombre|Edad|AnyoNac|FaltanJubilacion|  Ciudad|\n",
            "+---+------+----+-------+----------------+--------+\n",
            "|  0| Aitor|  45|   1980|              22|   Elche|\n",
            "|  1|Marina|  14|   2011|              53|Alicante|\n",
            "|  2| Laura|  19|   2006|              48|   Elche|\n",
            "|  3| Sonia|  45|   1980|              22|    Aspe|\n",
            "|  4| Pedro|NULL|   NULL|            NULL|   Elche|\n",
            "+---+------+----+-------+----------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "# Añadir columna de ID incremental\n",
        "df_nombres = df_nombres.withColumn(\"Id\", monotonically_increasing_id())\n",
        "\n",
        "# Reordenar las columnas\n",
        "df_nombres = df_nombres.select(\"Id\", \"Nombre\", \"Edad\", \"AnyoNac\", \"FaltanJubilacion\", \"Ciudad\")\n",
        "\n",
        "# Mostrar el resultado\n",
        "df_nombres.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZhUS4RU5SRd"
      },
      "source": [
        "## EJERCICIO 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0VRuSPS5SRd"
      },
      "source": [
        "A partir del archivo VentasNulos.csv:\n",
        "\n",
        "1. Elimina las filas que tengan al menos 4 nulos.\n",
        "\n",
        "2. Con las filas restantes, sustituye:\n",
        "\n",
        "    * Los nombres nulos por Empleado\n",
        "    * Las ventas nulas por la media de las ventas de los compañeros (redondeado a entero).\n",
        "    ``````\n",
        "        media = df.groupBy().avg('Ventas')\n",
        "    ``````\n",
        "    * Los euros nulos por el valor del compañero que menos € ha ganado. (tras agrupar, puedes usar la función min)\n",
        "    * La ciudad nula por C.V. y el identificador nulo por XYZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma0jMrYh5SRe",
        "outputId": "efe5c87e-8e98-45c3-96e3-d44befd97f6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------+-----+-----------+-------------+\n",
            "|Nombre|Ventas|Euros|     Ciudad|Identificador|\n",
            "+------+------+-----+-----------+-------------+\n",
            "|  Pepe|     4|  200|      Elche|          X21|\n",
            "|Andreu|     8| NULL|       NULL|         NULL|\n",
            "|  Juan|  NULL| NULL|       NULL|          C54|\n",
            "| Pedro|     1|   30|   Valencia|          R23|\n",
            "| María|  NULL|  300| Torrellano|         NULL|\n",
            "|Marina|     3|  350|       Aspe|          V55|\n",
            "|  NULL|    10|  500|Crevillente|          AMV|\n",
            "|   Ana|    10| 2300|   Alicante|          B89|\n",
            "|  NULL|  NULL| NULL|       NULL|         NULL|\n",
            "| Jorge|     8| NULL|       NULL|          T19|\n",
            "+------+------+-----+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_venta = spark.read.csv(\"recursos/VentasNulos.csv\", header=True, inferSchema=True)\n",
        "df_venta.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------+-----+-----------+-------------+\n",
            "|Nombre|Ventas|Euros|     Ciudad|Identificador|\n",
            "+------+------+-----+-----------+-------------+\n",
            "|  Pepe|     4|  200|      Elche|          X21|\n",
            "|Andreu|     8| NULL|       NULL|         NULL|\n",
            "|  Juan|  NULL| NULL|       NULL|          C54|\n",
            "| Pedro|     1|   30|   Valencia|          R23|\n",
            "| María|  NULL|  300| Torrellano|         NULL|\n",
            "|Marina|     3|  350|       Aspe|          V55|\n",
            "|  NULL|    10|  500|Crevillente|          AMV|\n",
            "|   Ana|    10| 2300|   Alicante|          B89|\n",
            "| Jorge|     8| NULL|       NULL|          T19|\n",
            "+------+------+-----+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Crear una expresión para contar los valores nulos en cada fila\n",
        "expr_nulos = \"\"\"\n",
        "    IF(Nombre IS NULL, 1, 0) + \n",
        "    IF(Ventas IS NULL, 1, 0) + \n",
        "    IF(Euros IS NULL, 1, 0) + \n",
        "    IF(Ciudad IS NULL, 1, 0) + \n",
        "    IF(Identificador IS NULL, 1, 0)\n",
        "\"\"\"\n",
        "\n",
        "# Filtrar las filas que tengan menos de 4 nulos\n",
        "df_venta_filtrado = df_venta.filter(expr(f\"({expr_nulos}) < 4\"))\n",
        "\n",
        "df_venta_filtrado.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+------+-----+-----------+-------------+\n",
            "|  Nombre|Ventas|Euros|     Ciudad|Identificador|\n",
            "+--------+------+-----+-----------+-------------+\n",
            "|    Pepe|   4.0|  200|      Elche|          X21|\n",
            "|  Andreu|   8.0|   30|       C.V.|          XYZ|\n",
            "|    Juan|   6.0|   30|       C.V.|          C54|\n",
            "|   Pedro|   1.0|   30|   Valencia|          R23|\n",
            "|   María|   6.0|  300| Torrellano|          XYZ|\n",
            "|  Marina|   3.0|  350|       Aspe|          V55|\n",
            "|Empleado|  10.0|  500|Crevillente|          AMV|\n",
            "|     Ana|  10.0| 2300|   Alicante|          B89|\n",
            "|   Jorge|   8.0|   30|       C.V.|          T19|\n",
            "+--------+------+-----+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import expr, avg, min, round\n",
        "\n",
        "# 1. Calcular la media de las ventas (redondeado a entero)\n",
        "media_ventas = df_venta_filtrado.agg(round(avg(\"Ventas\"), 0).alias(\"media_ventas\")).collect()[0][\"media_ventas\"]\n",
        "\n",
        "# 2. Calcular el valor mínimo de euros\n",
        "min_euros = df_venta_filtrado.agg(min(\"Euros\")).collect()[0][0]\n",
        "\n",
        "# 3. Sustituir los valores nulos en la columna Nombre por \"Empleado\"\n",
        "df_venta_filtrado_nombre = df_venta_filtrado.withColumn(\"Nombre\", \n",
        "    expr(\"CASE WHEN Nombre IS NULL THEN 'Empleado' ELSE Nombre END\"))\n",
        "\n",
        "# 4. Sustituir los valores nulos en la columna Ciudad por \"C.V.\"\n",
        "df_venta_filtrado_ciudad = df_venta_filtrado_nombre.withColumn(\"Ciudad\", \n",
        "    expr(\"CASE WHEN Ciudad IS NULL THEN 'C.V.' ELSE Ciudad END\"))\n",
        "\n",
        "# 5. Sustituir los valores nulos en la columna Identificador por \"XYZ\"\n",
        "df_venta_filtrado_identificador = df_venta_filtrado_ciudad.withColumn(\"Identificador\", \n",
        "    expr(\"CASE WHEN Identificador IS NULL THEN 'XYZ' ELSE Identificador END\"))\n",
        "\n",
        "# 6. Sustituir los valores nulos en la columna Ventas por la media de las ventas\n",
        "df_venta_filtrado_ventas = df_venta_filtrado_identificador.withColumn(\"Ventas\", \n",
        "    expr(f\"CASE WHEN Ventas IS NULL THEN {media_ventas} ELSE Ventas END\"))\n",
        "\n",
        "# 7. Sustituir los valores nulos en la columna Euros por el valor mínimo de euros\n",
        "df_venta_filtrado_euros = df_venta_filtrado_ventas.withColumn(\"Euros\", \n",
        "    expr(f\"CASE WHEN Euros IS NULL THEN {min_euros} ELSE Euros END\"))\n",
        "\n",
        "# Mostrar el resultado final\n",
        "df_venta_filtrado_euros.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62C3pGjg5SRe"
      },
      "source": [
        "## EJERCICIO 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz8oZ4zB5SRe"
      },
      "source": [
        " A partir del archivo movies.tsv, crea una esquema de forma declarativa con los campos:\n",
        "\n",
        "* interprete de tipo string\n",
        "* pelicula de tipo string\n",
        "* anyo de tipo int\n",
        "\n",
        "Cada fila del fichero implica que el actor/actriz ha trabajado en dicha película en el año indicado.\n",
        "1. Una vez creado el esquema, carga los datos en un DataFrame.\n",
        "\n",
        "A continuación, mediante el DataFrame API:\n",
        "\n",
        "2. Muestra las películas en las que ha trabajado Murphy, Eddie (I).\n",
        "3. Muestra los intérpretes que aparecen tanto en Superman como en Superman II."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Definir el esquema de forma declarativa\n",
        "esquema = StructType([\n",
        "    StructField(\"interprete\", StringType(), True),\n",
        "    StructField(\"pelicula\", StringType(), True),\n",
        "    StructField(\"anyo\", IntegerType(), True)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+---------------------------+----+\n",
            "|interprete       |pelicula                   |anyo|\n",
            "+-----------------+---------------------------+----+\n",
            "|McClure, Marc (I)|Freaky Friday              |2003|\n",
            "|McClure, Marc (I)|Coach Carter               |2005|\n",
            "|McClure, Marc (I)|Superman II                |1980|\n",
            "|McClure, Marc (I)|Apollo 13                  |1995|\n",
            "|McClure, Marc (I)|Superman                   |1978|\n",
            "|McClure, Marc (I)|Back to the Future         |1985|\n",
            "|McClure, Marc (I)|Back to the Future Part III|1990|\n",
            "|Cooper, Chris (I)|Me, Myself & Irene         |2000|\n",
            "|Cooper, Chris (I)|October Sky                |1999|\n",
            "|Cooper, Chris (I)|Capote                     |2005|\n",
            "|Cooper, Chris (I)|The Bourne Supremacy       |2004|\n",
            "|Cooper, Chris (I)|The Patriot                |2000|\n",
            "|Cooper, Chris (I)|The Town                   |2010|\n",
            "|Cooper, Chris (I)|Seabiscuit                 |2003|\n",
            "|Cooper, Chris (I)|A Time to Kill             |1996|\n",
            "|Cooper, Chris (I)|Where the Wild Things Are  |2009|\n",
            "|Cooper, Chris (I)|The Muppets                |2011|\n",
            "|Cooper, Chris (I)|American Beauty            |1999|\n",
            "|Cooper, Chris (I)|Syriana                    |2005|\n",
            "|Cooper, Chris (I)|The Horse Whisperer        |1998|\n",
            "+-----------------+---------------------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cargar el archivo movies.tsv con el esquema\n",
        "df_movies = spark.read.option(\"delimiter\", \"\\t\").schema(esquema).csv(\"recursos/movies.tsv\")\n",
        "\n",
        "# Mostrar el DataFrame resultante\n",
        "df_movies.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+---------------------------+----+\n",
            "|interprete       |pelicula                   |anyo|\n",
            "+-----------------+---------------------------+----+\n",
            "|McClure, Marc (I)|Freaky Friday              |2003|\n",
            "|McClure, Marc (I)|Coach Carter               |2005|\n",
            "|McClure, Marc (I)|Superman II                |1980|\n",
            "|McClure, Marc (I)|Apollo 13                  |1995|\n",
            "|McClure, Marc (I)|Superman                   |1978|\n",
            "|McClure, Marc (I)|Back to the Future         |1985|\n",
            "|McClure, Marc (I)|Back to the Future Part III|1990|\n",
            "|Cooper, Chris (I)|Me, Myself & Irene         |2000|\n",
            "|Cooper, Chris (I)|October Sky                |1999|\n",
            "|Cooper, Chris (I)|Capote                     |2005|\n",
            "|Cooper, Chris (I)|The Bourne Supremacy       |2004|\n",
            "|Cooper, Chris (I)|The Patriot                |2000|\n",
            "|Cooper, Chris (I)|The Town                   |2010|\n",
            "|Cooper, Chris (I)|Seabiscuit                 |2003|\n",
            "|Cooper, Chris (I)|A Time to Kill             |1996|\n",
            "|Cooper, Chris (I)|Where the Wild Things Are  |2009|\n",
            "|Cooper, Chris (I)|The Muppets                |2011|\n",
            "|Cooper, Chris (I)|American Beauty            |1999|\n",
            "|Cooper, Chris (I)|Syriana                    |2005|\n",
            "|Cooper, Chris (I)|The Horse Whisperer        |1998|\n",
            "+-----------------+---------------------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cargar los datos en un DataFrame con el esquema definido\n",
        "df_movies = spark.read.option(\"delimiter\", \"\\t\").schema(esquema).csv(\"recursos/movies.tsv\")\n",
        "\n",
        "# Mostrar el DataFrame cargado\n",
        "df_movies.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------+\n",
            "|pelicula                      |\n",
            "+------------------------------+\n",
            "|Showtime                      |\n",
            "|Norbit                        |\n",
            "|Hot Tub Time Machine          |\n",
            "|Nutty Professor II: The Klumps|\n",
            "|Beverly Hills Cop II          |\n",
            "|Trading Places                |\n",
            "|Daddy Day Care                |\n",
            "|Dr. Dolittle 2                |\n",
            "|Shrek Forever After           |\n",
            "|Beverly Hills Cop             |\n",
            "|Shrek                         |\n",
            "|The Haunted Mansion           |\n",
            "|Coming to America             |\n",
            "|Shrek 2                       |\n",
            "|Doctor Dolittle               |\n",
            "|The Nutty Professor           |\n",
            "|Mulan                         |\n",
            "|Tower Heist                   |\n",
            "|Dreamgirls                    |\n",
            "|Bowfinger                     |\n",
            "+------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Filtrar las filas donde el intérprete sea \"Murphy, Eddie (I)\"\n",
        "df_murphy = df_movies.filter(df_movies[\"interprete\"] == \"Murphy, Eddie (I)\")\n",
        "\n",
        "# Mostrar las películas en las que ha trabajado\n",
        "df_murphy.select(\"pelicula\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+\n",
            "|interprete        |\n",
            "+------------------+\n",
            "|O'Halloran, Jack  |\n",
            "|Tucker, Burnell   |\n",
            "|Hollis, John (I)  |\n",
            "|Beatty, Ned       |\n",
            "|Stamp, Terence    |\n",
            "|Ratzenberger, John|\n",
            "|Hackman, Gene     |\n",
            "|Fielder, Harry    |\n",
            "|Perrine, Valerie  |\n",
            "|McClure, Marc (I) |\n",
            "|Donner, Richard   |\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Filtrar las filas donde la película sea \"Superman\"\n",
        "df_superman = df_movies.filter(df_movies[\"pelicula\"] == \"Superman\")\n",
        "\n",
        "# Filtrar las filas donde la película sea \"Superman II\"\n",
        "df_superman_ii = df_movies.filter(df_movies[\"pelicula\"] == \"Superman II\")\n",
        "\n",
        "# Seleccionar los intérpretes de ambas películas\n",
        "intérpretes_superman = df_superman.select(\"interprete\")\n",
        "intérpretes_superman_ii = df_superman_ii.select(\"interprete\")\n",
        "\n",
        "# Hacer la intersección entre los intérpretes de ambas películas\n",
        "interseccion = intérpretes_superman.intersect(intérpretes_superman_ii)\n",
        "\n",
        "# Mostrar los intérpretes comunes\n",
        "interseccion.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhZ9aGeq5SRf"
      },
      "source": [
        "## EJERCICIO 5\n",
        "Realiza las siguientes operaciones:\n",
        "* Carga el dataset de “data/stocks_price_final.csv”, con el esquema correcto de datos (tienes que crear tu el schema\").\n",
        "* Renombra la variable market.cap a market\n",
        "* Elimina la variable market\n",
        "* Muestra las filas donde el valor de \"open\" es nulo.\n",
        "* Elimina las filas donde el valor de \"open\" es nulo.\n",
        "* Para comprobar el punto anterior vuelve a mostrar las filas donde el valor de \"open\" es nulo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oawpRKQ55SRf",
        "outputId": "21b4b449-0fe5-480f-f9bd-7b3e2e1ff6f4"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, LongType\n",
        "\n",
        "# Definir el esquema de forma declarativa\n",
        "esquema_price = StructType([\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"symbol\", StringType(), True),\n",
        "    StructField(\"date\", DateType(), True),\n",
        "    StructField(\"open\", FloatType(), True),\n",
        "    StructField(\"high\", FloatType(), True),\n",
        "    StructField(\"low\", FloatType(), True),\n",
        "    StructField(\"close\", FloatType(), True),\n",
        "    StructField(\"volume\", LongType(), True),\n",
        "    StructField(\"adjusted\", FloatType(), True),\n",
        "    StructField(\"market.cap\", StringType(), True),\n",
        "    StructField(\"sector\", StringType(), True),\n",
        "    StructField(\"industry\", StringType(), True),\n",
        "    StructField(\"exchange\", StringType(), True)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+------+----------+-----+------+------+-----+-------+--------+----------+-------------+--------------------+--------+\n",
            "|  id|symbol|      date| open|  high|   low|close| volume|adjusted|market.cap|       sector|            industry|exchange|\n",
            "+----+------+----------+-----+------+------+-----+-------+--------+----------+-------------+--------------------+--------+\n",
            "|NULL|symbol|      NULL| NULL|  NULL|  NULL| NULL|   NULL|    NULL|market.cap|       sector|            industry|exchange|\n",
            "|   1|   TXG|2019-09-12| 54.0|  58.0|  51.0|52.75|7326300|   52.75|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   2|   TXG|2019-09-13|52.75|54.355| 49.15|52.27|1025200|   52.27|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   3|   TXG|2019-09-16|52.45|  56.0| 52.01| 55.2| 269900|    55.2|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   4|   TXG|2019-09-17|56.21|  60.9|55.423|56.78| 602800|   56.78|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   5|   TXG|2019-09-18|56.85| 62.27| 55.65| 62.0|1589600|    62.0|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   6|   TXG|2019-09-19|62.81|63.375| 61.03|61.12| 425200|   61.12|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   7|   TXG|2019-09-20|61.71| 62.42| 59.82| 60.5| 392000|    60.5|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   8|   TXG|2019-09-23|60.22|61.485| 59.94|60.33| 137200|   60.33|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   9|   TXG|2019-09-24| 61.0|  61.0|  54.0| 54.3| 713800|    54.3|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  10|   TXG|2019-09-25|54.46| 55.88|52.563|52.76| 261200|   52.76|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  11|   TXG|2019-09-26|52.78| 53.69| 46.62|49.99| 596300|   49.99|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  12|   TXG|2019-09-27|51.13|  55.0|  50.7|51.03| 621300|   51.03|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  13|   TXG|2019-09-30|51.05|  52.0| 49.25| 50.4| 168900|    50.4|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  14|   TXG|2019-10-01|50.51| 51.92|  46.0|47.03| 536300|   47.03|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  15|   TXG|2019-10-02|46.78| 47.23| 45.11|46.07| 519600|   46.07|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  16|   TXG|2019-10-03|46.77| 48.24| 45.75|48.12| 703900|   48.12|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  17|   TXG|2019-10-04| 48.0| 53.34| 47.82|51.45| 322400|   51.45|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  18|   TXG|2019-10-07| 52.1| 53.22| 49.03|50.36| 476600|   50.36|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  19|   TXG|2019-10-08| 50.0| 51.27|  49.0|49.55| 284100|   49.55|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "+----+------+----------+-----+------+------+-----+-------+--------+----------+-------------+--------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_price = spark.read.csv(\"recursos/stocks_price_final.csv\", schema=esquema_price)\n",
        "df_price.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+------+----------+-----+------+------+-----+-------+--------+----------+-------------+--------------------+--------+\n",
            "|  id|symbol|      date| open|  high|   low|close| volume|adjusted|    market|       sector|            industry|exchange|\n",
            "+----+------+----------+-----+------+------+-----+-------+--------+----------+-------------+--------------------+--------+\n",
            "|NULL|symbol|      NULL| NULL|  NULL|  NULL| NULL|   NULL|    NULL|market.cap|       sector|            industry|exchange|\n",
            "|   1|   TXG|2019-09-12| 54.0|  58.0|  51.0|52.75|7326300|   52.75|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   2|   TXG|2019-09-13|52.75|54.355| 49.15|52.27|1025200|   52.27|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   3|   TXG|2019-09-16|52.45|  56.0| 52.01| 55.2| 269900|    55.2|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   4|   TXG|2019-09-17|56.21|  60.9|55.423|56.78| 602800|   56.78|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   5|   TXG|2019-09-18|56.85| 62.27| 55.65| 62.0|1589600|    62.0|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   6|   TXG|2019-09-19|62.81|63.375| 61.03|61.12| 425200|   61.12|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   7|   TXG|2019-09-20|61.71| 62.42| 59.82| 60.5| 392000|    60.5|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   8|   TXG|2019-09-23|60.22|61.485| 59.94|60.33| 137200|   60.33|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   9|   TXG|2019-09-24| 61.0|  61.0|  54.0| 54.3| 713800|    54.3|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  10|   TXG|2019-09-25|54.46| 55.88|52.563|52.76| 261200|   52.76|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  11|   TXG|2019-09-26|52.78| 53.69| 46.62|49.99| 596300|   49.99|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  12|   TXG|2019-09-27|51.13|  55.0|  50.7|51.03| 621300|   51.03|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  13|   TXG|2019-09-30|51.05|  52.0| 49.25| 50.4| 168900|    50.4|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  14|   TXG|2019-10-01|50.51| 51.92|  46.0|47.03| 536300|   47.03|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  15|   TXG|2019-10-02|46.78| 47.23| 45.11|46.07| 519600|   46.07|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  16|   TXG|2019-10-03|46.77| 48.24| 45.75|48.12| 703900|   48.12|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  17|   TXG|2019-10-04| 48.0| 53.34| 47.82|51.45| 322400|   51.45|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  18|   TXG|2019-10-07| 52.1| 53.22| 49.03|50.36| 476600|   50.36|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  19|   TXG|2019-10-08| 50.0| 51.27|  49.0|49.55| 284100|   49.55|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "+----+------+----------+-----+------+------+-----+-------+--------+----------+-------------+--------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Renombrar la columna market.cap a market\n",
        "df_renombrado = df_price.withColumnRenamed(\"market.cap\", \"market\")\n",
        "\n",
        "# Mostrar el DataFrame con el nuevo nombre de la columna\n",
        "df_renombrado.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+------+----------+-----+------+------+-----+-------+--------+-------------+--------------------+--------+\n",
            "|  id|symbol|      date| open|  high|   low|close| volume|adjusted|       sector|            industry|exchange|\n",
            "+----+------+----------+-----+------+------+-----+-------+--------+-------------+--------------------+--------+\n",
            "|NULL|symbol|      NULL| NULL|  NULL|  NULL| NULL|   NULL|    NULL|       sector|            industry|exchange|\n",
            "|   1|   TXG|2019-09-12| 54.0|  58.0|  51.0|52.75|7326300|   52.75|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   2|   TXG|2019-09-13|52.75|54.355| 49.15|52.27|1025200|   52.27|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   3|   TXG|2019-09-16|52.45|  56.0| 52.01| 55.2| 269900|    55.2|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   4|   TXG|2019-09-17|56.21|  60.9|55.423|56.78| 602800|   56.78|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   5|   TXG|2019-09-18|56.85| 62.27| 55.65| 62.0|1589600|    62.0|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   6|   TXG|2019-09-19|62.81|63.375| 61.03|61.12| 425200|   61.12|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   7|   TXG|2019-09-20|61.71| 62.42| 59.82| 60.5| 392000|    60.5|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   8|   TXG|2019-09-23|60.22|61.485| 59.94|60.33| 137200|   60.33|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|   9|   TXG|2019-09-24| 61.0|  61.0|  54.0| 54.3| 713800|    54.3|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  10|   TXG|2019-09-25|54.46| 55.88|52.563|52.76| 261200|   52.76|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  11|   TXG|2019-09-26|52.78| 53.69| 46.62|49.99| 596300|   49.99|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  12|   TXG|2019-09-27|51.13|  55.0|  50.7|51.03| 621300|   51.03|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  13|   TXG|2019-09-30|51.05|  52.0| 49.25| 50.4| 168900|    50.4|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  14|   TXG|2019-10-01|50.51| 51.92|  46.0|47.03| 536300|   47.03|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  15|   TXG|2019-10-02|46.78| 47.23| 45.11|46.07| 519600|   46.07|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  16|   TXG|2019-10-03|46.77| 48.24| 45.75|48.12| 703900|   48.12|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  17|   TXG|2019-10-04| 48.0| 53.34| 47.82|51.45| 322400|   51.45|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  18|   TXG|2019-10-07| 52.1| 53.22| 49.03|50.36| 476600|   50.36|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  19|   TXG|2019-10-08| 50.0| 51.27|  49.0|49.55| 284100|   49.55|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "+----+------+----------+-----+------+------+-----+-------+--------+-------------+--------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Eliminar la columna market\n",
        "df_sin_market = df_renombrado.drop(\"market\")\n",
        "\n",
        "# Mostrar el DataFrame sin la columna market\n",
        "df_sin_market.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+------+----------+----+----+----+-----+------+--------+-------------+--------------------+--------+\n",
            "|  id|symbol|      date|open|high| low|close|volume|adjusted|       sector|            industry|exchange|\n",
            "+----+------+----------+----+----+----+-----+------+--------+-------------+--------------------+--------+\n",
            "|NULL|symbol|      NULL|NULL|NULL|NULL| NULL|  NULL|    NULL|       sector|            industry|exchange|\n",
            "|4378|  KRKR|2020-05-11|NULL|NULL|NULL| NULL|  NULL|    NULL|Miscellaneous|   Business Services|  NASDAQ|\n",
            "|5747|  NMTR|2020-01-23|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5748|  NMTR|2020-01-24|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5749|  NMTR|2020-01-27|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5750|  NMTR|2020-01-28|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5751|  NMTR|2020-01-29|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5752|  NMTR|2020-01-30|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5753|  NMTR|2020-01-31|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5754|  NMTR|2020-02-03|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5755|  NMTR|2020-02-04|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5756|  NMTR|2020-02-05|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5757|  NMTR|2020-02-06|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5758|  NMTR|2020-02-07|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5759|  NMTR|2020-02-10|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5760|  NMTR|2020-02-11|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5761|  NMTR|2020-02-12|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5762|  NMTR|2020-02-13|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5763|  NMTR|2020-02-14|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5764|  NMTR|2020-02-18|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "+----+------+----------+----+----+----+-----+------+--------+-------------+--------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Filtrar filas donde el valor de \"open\" es nulo\n",
        "df_sin_market.filter(df_sin_market[\"open\"].isNull()).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+----------+-----+------+------+-----+-------+--------+-------------+--------------------+--------+\n",
            "| id|symbol|      date| open|  high|   low|close| volume|adjusted|       sector|            industry|exchange|\n",
            "+---+------+----------+-----+------+------+-----+-------+--------+-------------+--------------------+--------+\n",
            "|  1|   TXG|2019-09-12| 54.0|  58.0|  51.0|52.75|7326300|   52.75|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  2|   TXG|2019-09-13|52.75|54.355| 49.15|52.27|1025200|   52.27|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  3|   TXG|2019-09-16|52.45|  56.0| 52.01| 55.2| 269900|    55.2|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  4|   TXG|2019-09-17|56.21|  60.9|55.423|56.78| 602800|   56.78|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  5|   TXG|2019-09-18|56.85| 62.27| 55.65| 62.0|1589600|    62.0|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  6|   TXG|2019-09-19|62.81|63.375| 61.03|61.12| 425200|   61.12|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  7|   TXG|2019-09-20|61.71| 62.42| 59.82| 60.5| 392000|    60.5|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  8|   TXG|2019-09-23|60.22|61.485| 59.94|60.33| 137200|   60.33|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  9|   TXG|2019-09-24| 61.0|  61.0|  54.0| 54.3| 713800|    54.3|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 10|   TXG|2019-09-25|54.46| 55.88|52.563|52.76| 261200|   52.76|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 11|   TXG|2019-09-26|52.78| 53.69| 46.62|49.99| 596300|   49.99|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 12|   TXG|2019-09-27|51.13|  55.0|  50.7|51.03| 621300|   51.03|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 13|   TXG|2019-09-30|51.05|  52.0| 49.25| 50.4| 168900|    50.4|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 14|   TXG|2019-10-01|50.51| 51.92|  46.0|47.03| 536300|   47.03|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 15|   TXG|2019-10-02|46.78| 47.23| 45.11|46.07| 519600|   46.07|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 16|   TXG|2019-10-03|46.77| 48.24| 45.75|48.12| 703900|   48.12|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 17|   TXG|2019-10-04| 48.0| 53.34| 47.82|51.45| 322400|   51.45|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 18|   TXG|2019-10-07| 52.1| 53.22| 49.03|50.36| 476600|   50.36|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 19|   TXG|2019-10-08| 50.0| 51.27|  49.0|49.55| 284100|   49.55|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 20|   TXG|2019-10-09|49.63|51.525|49.575|50.01| 201100|   50.01|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "+---+------+----------+-----+------+------+-----+-------+--------+-------------+--------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Eliminar filas donde el valor de \"open\" es nulo\n",
        "df_sin_open_nulo = df_sin_market.filter(df_sin_market[\"open\"].isNotNull())\n",
        "\n",
        "# Mostrar el resultado\n",
        "df_sin_open_nulo.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Mostrar cuantas filas tienen el valor de \"open\" es nulo \n",
        "df_sin_open_nulo.filter(df_sin_open_nulo[\"open\"].isNull()).count()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
