{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Concatenate, Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que queremos predecir dos objetivos diferentes (por ejemplo, precio y categoría) a partir de dos conjuntos de características diferentes (por ejemplo, características visuales y características de texto de un producto).\n",
    "\n",
    "* Características visuales: un conjunto de 128 características numéricas, que podrían representar, por ejemplo, valores de píxeles o características extraídas de imágenes.\n",
    "* Características de texto: un conjunto de 256 características numéricas, representando posiblemente la codificación de texto o características lingüísticas.\n",
    "* Precio (Salida 1): un valor numérico que representa el precio de un producto.\n",
    "* Categoría (Salida 2): una etiqueta de categoría, que asumiremos que puede tomar 10 valores diferentes (por ejemplo, 10 categorías diferentes de productos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a generar datos aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generación de datos ficticios\n",
    "num_muestras = 1000  # Número de muestras en el conjunto de datos\n",
    "\n",
    "# Características visuales: 128 características numéricas\n",
    "caracteristicas_visuales = np.random.rand(num_muestras, 128)\n",
    "\n",
    "# Características de texto: 256 características numéricas\n",
    "caracteristicas_texto = np.random.rand(num_muestras, 256)\n",
    "\n",
    "# Precio (Salida 1): Valor numérico (por ejemplo, precio de un producto)\n",
    "precio = np.random.rand(num_muestras, 1)\n",
    "\n",
    "# Categoría (Salida 2): 10 categorías posibles (codificadas en one-hot)\n",
    "categorias = np.random.randint(0, 10, size=(num_muestras, 1))\n",
    "categorias_one_hot = tf.keras.utils.to_categorical(categorias, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide los datos en entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_visuales, X_test_visuales, X_train_texto, X_test_texto, y_train_precio, y_test_precio, y_train_categorias, y_test_categorias = train_test_split(\n",
    "    caracteristicas_visuales, \n",
    "    caracteristicas_texto, \n",
    "    precio, \n",
    "    categorias_one_hot, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construye el modelo:\n",
    "* Crea dos ramas de entrada, una para cada tipo de características.\n",
    "* Las ramas se fusionarán para predecir el precio.\n",
    "* Una de las ramas se utilizará también para predecir la categoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El esquema es el siguiente:\n",
    "\n",
    "<img src=\"esquema_modelo_funcional.png\" alt=\"esquema modelo funcional\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736956327.213100    7009 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9625 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_visual        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_texto         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ rama_visual (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ input_visual[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ rama_texto (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ input_texto[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ combinacion         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ rama_visual[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ rama_texto[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ salida_precio       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">193</span> │ combinacion[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ salida_categoria    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │ rama_texto[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_visual        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_texto         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ rama_visual (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ input_visual[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ rama_texto (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ input_texto[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ combinacion         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ rama_visual[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ rama_texto[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ salida_precio       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m193\u001b[0m │ combinacion[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ salida_categoria    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │      \u001b[38;5;34m1,290\u001b[0m │ rama_texto[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,635</span> (166.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m42,635\u001b[0m (166.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,635</span> (166.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m42,635\u001b[0m (166.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definimos las entradas\n",
    "input_visual = Input(shape=(128,), name=\"input_visual\")\n",
    "input_texto = Input(shape=(256,), name=\"input_texto\")\n",
    "\n",
    "# Rama visual\n",
    "rama_visual = Dense(64, activation=\"relu\", name=\"rama_visual\")(input_visual)\n",
    "\n",
    "# Rama textual\n",
    "rama_texto = Dense(128, activation=\"relu\", name=\"rama_texto\")(input_texto)\n",
    "\n",
    "# Combinación de las dos ramas\n",
    "combinacion = Concatenate(name=\"combinacion\")([rama_visual, rama_texto])\n",
    "\n",
    "# Salida para predecir el precio\n",
    "salida_precio = Dense(1, activation=\"linear\", name=\"salida_precio\")(combinacion)\n",
    "\n",
    "# Salida para predecir la categoría (usando la rama textual)\n",
    "salida_categoria = Dense(10, activation=\"softmax\", name=\"salida_categoria\")(rama_texto)\n",
    "\n",
    "# Creación del modelo\n",
    "modelo = Model(\n",
    "    inputs=[input_visual, input_texto],\n",
    "    outputs=[salida_precio, salida_categoria]\n",
    ")\n",
    "\n",
    "# Resumen del modelo\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprueba creando una imagen del modelo que es igual que el que se pide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La imagen del modelo se ha creado y guardado como 'modelo_funcional.png'.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Crear la imagen del modelo y guardarla en un archivo\n",
    "plot_model(\n",
    "    modelo, \n",
    "    to_file=\"modelo_funcional.png\",  # Nombre del archivo de salida\n",
    "    show_shapes=True,                # Mostrar las formas de los tensores en la imagen\n",
    "    show_layer_names=True,           # Mostrar los nombres de las capas\n",
    "    dpi=96                           # Resolución de la imagen\n",
    ")\n",
    "\n",
    "print(\"La imagen del modelo se ha creado y guardado como 'modelo_funcional.png'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"modelo_funcional.png\" alt=\"mi modelo funcional\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compila el modelo especificando las pérdidas y métricas para cada salida (utiliza el optimizador Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación del modelo\n",
    "modelo.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss={\n",
    "        \"salida_precio\": \"mae\",\n",
    "        \"salida_categoria\": \"categorical_crossentropy\"\n",
    "    },\n",
    "    metrics={\n",
    "        \"salida_precio\": \"mae\",\n",
    "        \"salida_categoria\": \"accuracy\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrena el modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1736956642.290928    7088 service.cc:148] XLA service 0x74be58004580 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1736956642.291082    7088 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4080 Laptop GPU, Compute Capability 8.9\n",
      "2025-01-15 16:57:22.316587: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1736956642.416525    7088 cuda_dnn.cc:529] Loaded cuDNN version 90600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/20\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 2s/step - loss: 3.2679 - salida_categoria_accuracy: 0.0625 - salida_categoria_loss: 2.5173 - salida_precio_loss: 0.7506 - salida_precio_mae: 0.7506"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736956643.200879    7088 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 2.8816 - salida_categoria_accuracy: 0.0959 - salida_categoria_loss: 2.4234 - salida_precio_loss: 0.4582 - salida_precio_mae: 0.4582 - val_loss: 2.6732 - val_salida_categoria_accuracy: 0.0875 - val_salida_categoria_loss: 2.3496 - val_salida_precio_loss: 0.3236 - val_salida_precio_mae: 0.3236\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5751 - salida_categoria_accuracy: 0.1408 - salida_categoria_loss: 2.2913 - salida_precio_loss: 0.2838 - salida_precio_mae: 0.2838 - val_loss: 2.6426 - val_salida_categoria_accuracy: 0.1000 - val_salida_categoria_loss: 2.3205 - val_salida_precio_loss: 0.3222 - val_salida_precio_mae: 0.3222\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.5268 - salida_categoria_accuracy: 0.1652 - salida_categoria_loss: 2.2514 - salida_precio_loss: 0.2755 - salida_precio_mae: 0.2755 - val_loss: 2.6243 - val_salida_categoria_accuracy: 0.1125 - val_salida_categoria_loss: 2.3124 - val_salida_precio_loss: 0.3119 - val_salida_precio_mae: 0.3119\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4772 - salida_categoria_accuracy: 0.1780 - salida_categoria_loss: 2.2208 - salida_precio_loss: 0.2564 - salida_precio_mae: 0.2564 - val_loss: 2.6542 - val_salida_categoria_accuracy: 0.0812 - val_salida_categoria_loss: 2.3452 - val_salida_precio_loss: 0.3089 - val_salida_precio_mae: 0.3089\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4292 - salida_categoria_accuracy: 0.1975 - salida_categoria_loss: 2.2053 - salida_precio_loss: 0.2239 - salida_precio_mae: 0.2239 - val_loss: 2.6600 - val_salida_categoria_accuracy: 0.0750 - val_salida_categoria_loss: 2.3374 - val_salida_precio_loss: 0.3226 - val_salida_precio_mae: 0.3226\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3704 - salida_categoria_accuracy: 0.2752 - salida_categoria_loss: 2.1570 - salida_precio_loss: 0.2134 - salida_precio_mae: 0.2134 - val_loss: 2.6433 - val_salida_categoria_accuracy: 0.0562 - val_salida_categoria_loss: 2.3360 - val_salida_precio_loss: 0.3073 - val_salida_precio_mae: 0.3073\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3409 - salida_categoria_accuracy: 0.2580 - salida_categoria_loss: 2.1309 - salida_precio_loss: 0.2099 - salida_precio_mae: 0.2099 - val_loss: 2.6985 - val_salida_categoria_accuracy: 0.0625 - val_salida_categoria_loss: 2.3943 - val_salida_precio_loss: 0.3042 - val_salida_precio_mae: 0.3042\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3114 - salida_categoria_accuracy: 0.2451 - salida_categoria_loss: 2.1112 - salida_precio_loss: 0.2002 - salida_precio_mae: 0.2002 - val_loss: 2.6618 - val_salida_categoria_accuracy: 0.0625 - val_salida_categoria_loss: 2.3470 - val_salida_precio_loss: 0.3148 - val_salida_precio_mae: 0.3148\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2526 - salida_categoria_accuracy: 0.3172 - salida_categoria_loss: 2.0612 - salida_precio_loss: 0.1914 - salida_precio_mae: 0.1914 - val_loss: 2.6873 - val_salida_categoria_accuracy: 0.0875 - val_salida_categoria_loss: 2.3517 - val_salida_precio_loss: 0.3355 - val_salida_precio_mae: 0.3355\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.2147 - salida_categoria_accuracy: 0.2900 - salida_categoria_loss: 2.0304 - salida_precio_loss: 0.1843 - salida_precio_mae: 0.1843 - val_loss: 2.7100 - val_salida_categoria_accuracy: 0.0688 - val_salida_categoria_loss: 2.4061 - val_salida_precio_loss: 0.3039 - val_salida_precio_mae: 0.3039\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "historial = modelo.fit(\n",
    "    {\"input_visual\": X_train_visuales, \"input_texto\": X_train_texto},\n",
    "    {\"salida_precio\": y_train_precio, \"salida_categoria\": y_train_categorias},\n",
    "    validation_split=0.2,   # Dividir un 20% del entrenamiento para validación\n",
    "    epochs=10,              # Número de épocas\n",
    "    batch_size=32,          # Tamaño de batch\n",
    "    verbose=1               # Mostrar el progreso del entrenamiento\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalúa el modelo en el conjunt de pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 2.7044 - salida_categoria_accuracy: 0.0948 - salida_categoria_loss: 2.3990 - salida_precio_loss: 0.3109 - salida_precio_mae: 0.3088\n",
      "\n",
      "Resultados de evaluación:\n",
      "Pérdida total: 2.672969102859497\n",
      "Pérdida precio: 0.30701833963394165\n",
      "MAE precio: 2.3880646228790283\n",
      "Pérdida categoría: 0.10999999940395355\n",
      "Precisión categoría: 0.29883885383605957\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo con el conjunto de pruebas\n",
    "resultados = modelo.evaluate(\n",
    "    {\"input_visual\": X_test_visuales, \"input_texto\": X_test_texto},\n",
    "    {\"salida_precio\": y_test_precio, \"salida_categoria\": y_test_categorias},\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\nResultados de evaluación:\")\n",
    "print(f\"Pérdida total: {resultados[0]}\")\n",
    "print(f\"Pérdida precio: {resultados[1]}\")\n",
    "print(f\"MAE precio: {resultados[2]}\")\n",
    "print(f\"Pérdida categoría: {resultados[3]}\")\n",
    "print(f\"Precisión categoría: {resultados[4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este ejercicio es entrenar un modelo en un subconjunto de clases (Modelo A) y luego usar este modelo para entrenar otro modelo en un subconjunto diferente de clases (Modelo B), primero sin y luego con el conocimiento transferido del Modelo A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga los datos de CIFAR-10, representa alguna de sus imágenes con sus etiquetas, haz una lista con las etiquetas que tiene (busca en internet) y normaliza los datos dividiendo entre 255.0.\n",
    "\n",
    "Divide los datos en conjuntos de entrenamiento, pruebas y validación para el modelo A y para el modelo B:\n",
    "* El modelo A utiliza las clases \"avión\", \"automóvil\", \"pájaro\" y \"gato\"\n",
    "* El modelo B utiliza las clases \"ciervo\", \"perro\", \"rana\" y \"caballo\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cargar el conjunto de datos CIFAR-10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Etiquetas de CIFAR-10\n",
    "labels = [\n",
    "    'avión', 'automóvil', 'pájaro', 'gato', 'ciervo', \n",
    "    'perro', 'rana', 'caballo', 'barco', 'camión'\n",
    "]\n",
    "\n",
    "# Mostrar una imagen del conjunto de entrenamiento con su etiqueta\n",
    "plt.imshow(x_train[0])\n",
    "plt.title(labels[y_train[0][0]])  # Obtener la etiqueta de la imagen\n",
    "plt.show()\n",
    "\n",
    "# Normalizar los datos (dividir entre 255.0)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "\n",
    "# Filtrar las clases para el Modelo A (clases: 'avión', 'automóvil', 'pájaro', 'gato') - clases 0, 1, 2, 3\n",
    "train_mask_A = np.isin(y_train, [0, 1, 2, 3]).flatten()\n",
    "test_mask_A = np.isin(y_test, [0, 1, 2, 3]).flatten()\n",
    "\n",
    "x_train_A = x_train[train_mask_A]\n",
    "y_train_A = y_train[train_mask_A]\n",
    "x_test_A = x_test[test_mask_A]\n",
    "y_test_A = y_test[test_mask_A]\n",
    "\n",
    "# Filtrar las clases para el Modelo B (clases: 'ciervo', 'perro', 'rana', 'caballo') - clases 4, 5, 6, 7\n",
    "train_mask_B = np.isin(y_train, [4, 5, 6, 7]).flatten()\n",
    "test_mask_B = np.isin(y_test, [4, 5, 6, 7]).flatten()\n",
    "\n",
    "x_train_B = x_train[train_mask_B]\n",
    "y_train_B = y_train[train_mask_B]\n",
    "x_test_B = x_test[test_mask_B]\n",
    "y_test_B = y_test[test_mask_B]\n",
    "\n",
    "# Dividir los datos de entrenamiento en conjuntos de entrenamiento, validación y prueba para el Modelo A\n",
    "x_train_A, x_val_A, y_train_A, y_val_A = train_test_split(x_train_A, y_train_A, test_size=0.1, random_state=42)\n",
    "x_train_A, x_test_A, y_train_A, y_test_A = train_test_split(x_train_A, y_train_A, test_size=0.1, random_state=42)\n",
    "\n",
    "# Dividir los datos de entrenamiento en conjuntos de entrenamiento, validación y prueba para el Modelo B\n",
    "x_train_B, x_val_B, y_train_B, y_val_B = train_test_split(x_train_B, y_train_B, test_size=0.1, random_state=42)\n",
    "x_train_B, x_test_B, y_train_B, y_test_B = train_test_split(x_train_B, y_train_B, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrena el Modelo A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iabd/anaconda3/envs/sapa/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.5214 - loss: 1.0510 - val_accuracy: 0.7240 - val_loss: 0.6965\n",
      "Epoch 2/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7288 - loss: 0.6852 - val_accuracy: 0.7450 - val_loss: 0.6603\n",
      "Epoch 3/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7602 - loss: 0.6104 - val_accuracy: 0.7520 - val_loss: 0.6065\n",
      "Epoch 4/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7847 - loss: 0.5447 - val_accuracy: 0.7745 - val_loss: 0.5617\n",
      "Epoch 5/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8075 - loss: 0.4914 - val_accuracy: 0.7980 - val_loss: 0.5210\n",
      "Epoch 6/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8216 - loss: 0.4587 - val_accuracy: 0.8080 - val_loss: 0.4786\n",
      "Epoch 7/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8377 - loss: 0.4176 - val_accuracy: 0.8065 - val_loss: 0.4842\n",
      "Epoch 8/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8542 - loss: 0.3826 - val_accuracy: 0.8140 - val_loss: 0.4879\n",
      "Epoch 9/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8621 - loss: 0.3586 - val_accuracy: 0.8130 - val_loss: 0.5163\n",
      "Epoch 10/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8735 - loss: 0.3209 - val_accuracy: 0.7830 - val_loss: 0.5662\n",
      "57/57 - 1s - 17ms/step - accuracy: 0.7939 - loss: 0.5587\n",
      "Modelo A - Precisión en el conjunto de prueba: 0.7939\n"
     ]
    }
   ],
   "source": [
    "# Crear el modelo para el Modelo A\n",
    "model_A = Sequential([\n",
    "    # Capa de convolución con 32 filtros, tamaño de kernel (3x3) y activación ReLU\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Capa de convolución con 64 filtros, tamaño de kernel (3x3) y activación ReLU\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Capa de convolución con 64 filtros, tamaño de kernel (3x3) y activación ReLU\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    \n",
    "    # Capa de aplanado\n",
    "    Flatten(),\n",
    "    \n",
    "    # Capa densa de 64 neuronas con activación ReLU\n",
    "    Dense(64, activation='relu'),\n",
    "    \n",
    "    # Capa de salida con 4 neuronas (para 4 clases) y activación softmax\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model_A.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history_A = model_A.fit(x_train_A, y_train_A, epochs=10, batch_size=64, validation_data=(x_val_A, y_val_A))\n",
    "\n",
    "# Evaluar el modelo en los datos de prueba\n",
    "test_loss, test_acc = model_A.evaluate(x_test_A, y_test_A, verbose=2)\n",
    "\n",
    "print(f\"Modelo A - Precisión en el conjunto de prueba: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar el Modelo B desde 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 2/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 3/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 4/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 5/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 6/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 7/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 8/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 9/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 10/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "57/57 - 0s - 7ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Modelo B - Precisión en el conjunto de prueba: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Crear el modelo para el Modelo B\n",
    "model_B = Sequential([\n",
    "    # Capa de convolución con 32 filtros, tamaño de kernel (3x3) y activación ReLU\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Capa de convolución con 64 filtros, tamaño de kernel (3x3) y activación ReLU\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Capa de convolución con 64 filtros, tamaño de kernel (3x3) y activación ReLU\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    \n",
    "    # Capa de aplanado\n",
    "    Flatten(),\n",
    "    \n",
    "    # Capa densa de 64 neuronas con activación ReLU\n",
    "    Dense(64, activation='relu'),\n",
    "    \n",
    "    # Capa de salida con 4 neuronas (para 4 clases) y activación softmax\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model_B.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history_B = model_B.fit(x_train_B, y_train_B, epochs=10, batch_size=64, validation_data=(x_val_B, y_val_B))\n",
    "\n",
    "# Evaluar el modelo en los datos de prueba\n",
    "test_loss_B, test_acc_B = model_B.evaluate(x_test_B, y_test_B, verbose=2)\n",
    "\n",
    "print(f\"Modelo B - Precisión en el conjunto de prueba: {test_acc_B:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar el Modelo B utilizando las capas aprendidas del Modelo A (excepto la capa de salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 2/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 3/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 4/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 5/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 6/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 7/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 8/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 9/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 10/10\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "57/57 - 0s - 7ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Modelo B con transferencia de conocimiento - Precisión en el conjunto de prueba: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Crear el modelo para el Modelo B utilizando transferencia de conocimiento del Modelo A\n",
    "model_B_transfer = Sequential([\n",
    "    # Capas convolucionales del Modelo A (congeladas)\n",
    "    model_A.layers[0],  # Capa Conv2D 1\n",
    "    model_A.layers[1],  # Capa MaxPooling2D 1\n",
    "    model_A.layers[2],  # Capa Conv2D 2\n",
    "    model_A.layers[3],  # Capa MaxPooling2D 2\n",
    "    model_A.layers[4],  # Capa Conv2D 3\n",
    "    \n",
    "    # Capa de aplanado\n",
    "    model_A.layers[5],  # Capa Flatten\n",
    "    \n",
    "    # Capa densa de 64 neuronas con activación ReLU\n",
    "    model_A.layers[6],  # Capa Dense 64\n",
    "    \n",
    "    # Nueva capa de salida con 4 neuronas (para las 4 clases de Modelo B)\n",
    "    Dense(4, activation='softmax')  # Cambiar el número de clases\n",
    "])\n",
    "\n",
    "# Congelar las capas convolucionales y densas preentrenadas del Modelo A\n",
    "for layer in model_B_transfer.layers[:-1]:\n",
    "    layer.trainable = False  # No entrenar las capas previas\n",
    "\n",
    "# Compilar el modelo\n",
    "model_B_transfer.compile(optimizer='adam',\n",
    "                         loss='sparse_categorical_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo B con las capas preentrenadas\n",
    "history_B_transfer = model_B_transfer.fit(x_train_B, y_train_B, epochs=10, batch_size=64, validation_data=(x_val_B, y_val_B))\n",
    "\n",
    "# Evaluar el modelo en los datos de prueba\n",
    "test_loss_B_transfer, test_acc_B_transfer = model_B_transfer.evaluate(x_test_B, y_test_B, verbose=2)\n",
    "\n",
    "print(f\"Modelo B con transferencia de conocimiento - Precisión en el conjunto de prueba: {test_acc_B_transfer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluar los dos modelos para el conjunto de datos B en el conjunto de pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 - 0s - 2ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Modelo A en el conjunto de datos B - Precisión: 0.0000\n",
      "57/57 - 0s - 2ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Modelo B (entrenado desde cero) en el conjunto de datos B - Precisión: 0.0000\n",
      "57/57 - 0s - 2ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Modelo B con transferencia de conocimiento en el conjunto de datos B - Precisión: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el Modelo A en el conjunto de datos B (Modelo B en el conjunto de prueba)\n",
    "test_loss_A_B, test_acc_A_B = model_A.evaluate(x_test_B, y_test_B, verbose=2)\n",
    "print(f\"Modelo A en el conjunto de datos B - Precisión: {test_acc_A_B:.4f}\")\n",
    "\n",
    "# Evaluar el Modelo B (entrenado desde cero) en el conjunto de datos B\n",
    "test_loss_B_B, test_acc_B_B = model_B.evaluate(x_test_B, y_test_B, verbose=2)\n",
    "print(f\"Modelo B (entrenado desde cero) en el conjunto de datos B - Precisión: {test_acc_B_B:.4f}\")\n",
    "\n",
    "# Evaluar el Modelo B con transferencia de conocimiento en el conjunto de datos B\n",
    "test_loss_B_transfer, test_acc_B_transfer = model_B_transfer.evaluate(x_test_B, y_test_B, verbose=2)\n",
    "print(f\"Modelo B con transferencia de conocimiento en el conjunto de datos B - Precisión: {test_acc_B_transfer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿En este caso merece la pena?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sapa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nav_menu": {
   "height": "360px",
   "width": "416px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
